<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="behavioral data collection, mechanical turk, psychology">
    <meta name="author" content="todd m. gureckis">


    <title></title>
    <script src="http://code.jquery.com/jquery-1.10.1.min.js"></script> 
    <script src="http://d3js.org/d3.v3.min.js" charset="utf-8"></script>
    <script src="js/remark.js" type="text/javascript"></script>


    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/cdbo.css" rel="stylesheet">


  </head>
  <body>
    <textarea id="source">
name: title
class: center, middle


# Conducting Replicable Behavioral Experiments over the Internet
<hr>
.author[

**Todd Gureckis**, Jay Martin, Alex Rich, John McDonnell, 
Anna Coenen, Doug Markant, and many github contributors! 

_[Computation and Cognition Lab](http://gureckislab.org)_, _New York University_  
]

---

class: middle

.task[Data on the Mind Workshop - UCBerkeley]


.left-column[
<br><br><br><br><br>
<img src="images/tools.JPG" width="300"> 
]

.right-column[
# Getting bigger data into psychology demands new tools
<hr>

1. Tool building enables widespread discovery
1. Lowers cost of entry for researchers for diverse technical backgrounds
1. Can help promote adoption of better research practices and replication
1. .blue[Open source] tools limit bugs
]

???

Starting with the quesiton of how
big data can make an impact on psychology, 
quick overview is going to focus on tool building and
infrastructure development

I think we all recognize the important of tool building in this
effort, but particularly I am interested in how building software
tool can help promote the adoption of better research practices
and replication effors.

---

class: middle
 
 .task[Data on the Mind Workshop - UCBerkeley]

.left-column[
 
<img src="images/Computer_Workstation_Variables.jpg" width="300">
]
 
.right-column[
# typical laboratory-based experiment
<hr>
 
1. Office-style interaction
1. General control over lighting, temperature, viewing angle, distance
1. A few people at a time (1-5/hour), each on individual workstations
1. Data saved to local disc or to a local file server
]

???

In particular I've been focused lately on how to expand the
range of experiments we conduct in psychology.  To that end
we can first consider a typical laboratory-based experiment

---

class: middle

.task[Data on the Mind Workshop - UCBerkeley]


.left-column[
.center[
<img src="images/womenonlaptop.jpg" width="200"><img src="images/TabletUser.jpg" width="200">
<img src="images/smartphone.jpg" width="200"><img src="images/OutsideUser.jpg" width="200">
]
]

.right-column[
# typical online experiment
<hr>

1. Variable lighting, conditions, computer system, etc...
1. International subject pool
1. Many people at the same time (10-100/hour), each talking to a centralized server
1. Data saved in database to allow concurrent reading/writing

]

???

and we can contrast that with a typical online experiment,
meaning one where a participant completes some traditional psychology
experiment over the Internet. Here the norm is variable lighting, etc...

Almost all of these features bring with them corresponding technical
challenges which will be the focus of the talk

---

name: replicationstudies

.task[Data on the Mind Workshop - UCBerkeley]

# How does online data compare to that collected in the lab?

.refs[

- Gosling, S.D., Vazire, S., Srivastava, S., & John, O.P. (2004). [Should we trust web-based studies? A comparative analysis of six preconceptions about Internet questionnaires](http://ww.w.simine.com/docs/Gosling_et_al_AP_2004.pdf). _American Psychologist_, 59, 2, 93-104.

- Paolacci, G., Chandler, J., & Ipeirotis, P. G. (2010). [Running experiments on Amazon Mechanical Turk](http://repub.eur.nl/res/pub/31983/jdm10630a[1].pdf). _Judgment and Decision Making_, 5, 411-419.

- Buhrmester, M., Kwang, T., & Gosling, S. D. (2011). [Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data?](http://pps.sagepub.com/content/6/1/3.full). _Perspectives on Psychological Science_, 6(1), 3-5.

- Germine, L., Nakayama, K., Duchaine, B.C., Chabris, C.F., Chatterjee, G. & Wilmer, J.B. (2012).  [Is the Web as good as the lab?  Comparable performance from Web and lab in cognitive/perceptual experiments](http://www.springerlink.com/content/f0244t772070138w/)  _Psychonomic Bulletin & Review_,  19.5.


- Shapiro, D. N., Chandler, J., & Mueller, P. A. (2013). [Using Mechanical Turk to Study Clinical Populations](http://s3.amazonaws.com/academia.edu.documents/30554524/Clinical_Psychological_Science-2013-Shapiro-2167702612469015.pdf?AWSAccessKeyId=AKIAIR6FSIMDFXPEERSA&Expires=1374090987&Signature=%2B4nErhKWOQhoWYY9gpgV0EbvVa0%3D&response-content-disposition=inline). _Clinical Psychological Science_, 1(2), 213-220.

]


???

one natural quesiton is how data collected over the Internet
compares to data collected in the lab, and there has been a 
lot of interest in this in the field lately.

---

.task[Data on the Mind Workshop - UCBerkeley]

# Do classic findings replicate?

- Some members of our lab recently contributed to this study that tested some classic psychology experiments on AMT

    - Crump, M. J., McDonnell, J. V., & Gureckis, T. M. (2013). [Evaluating Amazon's Mechanical Turk as a tool for experimental behavioral research.](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0057410) PloS one, 8(3).


- Focus was on reaction time findings that require sustained attention from subjects and precise recording of responses

- And learning experiments involving more cognitive effort 

--
# Basic conclusion was yes, many thing replicate very well using online samples.red[*]


.footnote[.red[*] With some caveats which happy to talk about later! For a summary of the paper see this [blog post](http://gureckislab.org/blog/?p=1297)]


???

my lab looked at this in the context of traditioan cognitive
experiments which require sustained attention and measurement
of things like reaction time, and in a big replication study we showed
that many findings do replicate successfully, although there were some
interesting exceptions that highlight the important of "best practices"
of conducting online studies.

---

.task[Data on the Mind Workshop - UCBerkeley]

# Amazon .blue[Mechanical Turk]


- Developed by [Amazon.com](http://amazon.com)

- Originally for in-house use to detect duplicate product postings on Amazon's site .red[*]

<br><br>
.center[<img src="images/AMTLogo.png" width="300">
]

# Key terminology


- **HIT** = Human Intelligence Task (a unit of work, e.g. a trial or an entire sequence of trials in an experiment)

- .orange[**Requester**] = an entity (e.g., researcher) who posts HITs

- .blue[**Worker**] = a person who performs the task


.footnote[.red[*] Nice summary in the [New York Times](http://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html)]

???

without a doubt the most popular platform for online data collection in
psychology is Amazon's Mechanical Turk.  I won't go into it too mu
---

.task[Data on the Mind Workshop - UCBerkeley]

# The challenge
 
- You are responsible for:
    - Hosting the experiment
       - **At minimum** you need a computer with a web-accessible HTTP server running (a static IP address may simplify things)

    - Posting your link to Amazon
     
    - Communicating back to Amazon when the HIT has been completed
     
    - Approving completed HITs for payment in a timely manner

    - Assigning bonuses
     
    - Dealing quickly with worker issues
     
    - Saving your data in a secure place, etc..

---

.task[Data on the Mind Workshop - UCBerkeley]

# The state of the field 2014
 
- Lots of labs are using AMT or generally online data collection

- Every lab implements their own approach to this problem (scripts, workflows)

- Sometimes ever *member* within a lab uses their own approach

- Running someone else's experiment takes lots of documentation and email
back and forth

--
# This gives up away one of the major advantages of online data collection: .blue[painless and precise replication]!


---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/psiturklogo_caption.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/what_is_1.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/what_is_2.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/what_is_3.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/what_is_4.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/what_is_5.png" width="730">

---

.task[Data on the Mind Workshop - UCBerkeley]

# Architecture

- **Command line tool** (this is the "player")
  - Allows you run experiments from a computer of your choosing
  - Allows you to interact with AMT to pay people, assign bonuses

- **psiturk.org Cloud-based services** 
  - provides secure hosting (Ad Server)
  - experiment exchange (system for sharing experiments between researchers)

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_1.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_2.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_3.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_4.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_5.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_6.png" width="730">

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/cloud_7.png" width="730">

---

.task[Data on the Mind Workshop - UCBerkeley]
# Installation

**System requirements **

- psiTurk works only on OS X and Linux. Windows is not supported (cloud-based options available!).

<br>
**Software dependencies**    

- **Python**

  - If you've never used Python before, we recommend you use the [Enthought Python distribution](http://www.enthought.com/repo/.epd_academic_installers), 
  because it comes with pip, a tool for easily installing Python packages. 

  <br>
  <br>

---

.task[Data on the Mind Workshop - UCBerkeley]

**Installing psiTurk**

- To install psiTurk simply run:

```bash
pip install psiturk
# If you get a permissions error try
# sudo pip install psiturk
```

<hr>

**Installing someone else's experiment**

- Visit the experiment exchange ( .blue[http://psiturk.org/ee ])  - like an "App Store" for experiments
- To download simply lookup the unique code and:

```bash
psiturk-install FobiuxfN33TiqGb7poQKvN
```

- Make a few changes to some config files and you can be
replicating within 10 minutes

- Workflow the same for all experiments regardless of content

---

.task[Data on the Mind Workshop - UCBerkeley]

# Command line features

**1. Experiment server**

  - Hosts lab and web experiments
  - Provides database and websserver functionality
  - Blocks repeat participants

**2. Interactive shell **

  - Interface for Amazon
  - post ads, pay workers, change settings, etc.
  - Launch and debug code

**3. Javascript library**

  - Provides common experiment functionality
  - save data to server, load content, log window changes

---


class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

# What have we learned?

---


class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

# psiTurk by the numbers
<img src="images/psiturknumbers.png" width="580">

---


class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/workdistribution.png">

---


class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

<img src="images/experimentsizes.png">

---

.task[Data on the Mind Workshop - UCBerkeley]

# What have we learned?

**1. Building .blue[data science] tools is really hard**

  - Barriers to adoption (inertia, "i didn't program it I don't trust it"), IRBs
  - Hard to document everything... a major effort spent in lab
  - Developing in the open means people get fustrated if current version doesn't work

--

**2. Building .blue[data science] tool is really rewarding**

  - People all over the world interested in your work
  - First time someone outside your lab contributes a new feature it is exciting
  - Get better ideas, opportunities all the time

--

**3. Big data possibility still isn't that "big"**

  - Traditional experiment sizes still common

---

class: center, middle

.task[Data on the Mind Workshop - UCBerkeley]

### Visit psiturk.org, chat with me!
# Thanks!



    </textarea>
    <script type="text/javascript">

      function getSearchParameters() {
            var prmstr = window.location.search.substr(1);
            return prmstr != null && prmstr != "" ? transformToAssocArray(prmstr) : {};
      }

      function transformToAssocArray( prmstr ) {
          var params = {};
          var prmarr = prmstr.split("&");
          for ( var i = 0; i < prmarr.length; i++) {
              var tmparr = prmarr[i].split("=");
              params[tmparr[0]] = tmparr[1];
          }
          return params;
      }

      var params = getSearchParameters();

      // this loads the markdown slide content from a file
      // $.ajaxSetup({async:false});
      // $("#source").load("slides.md");
      var slideshow = remark.alloc();
      slideshow.start();

    </script>

  </body>
</html>
