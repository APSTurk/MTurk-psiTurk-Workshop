<!DOCTYPE html>
<html>
  <head>
    <title>Using Mechanical Turk for Cognition Experiments - John McDonnell and Todd Gureckis</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style rel="stylesheet" href="styles/monokai.css">
    <style type="text/css" media="screen">
    /* Slideshow styles */
      @import url(http://fonts.googleapis.com/css?family=Lobster);
      @import url(http://fonts.googleapis.com/css?family=Open+Sans);
      @import url(http://fonts.googleapis.com/css?family=Oxygen);
      
      body { 
        font-family: 'Oxygen', sans-serif;
        font-size: 24px;
      }
      h1 {
          font-family: 'Lobster', cursive;
          color: #444;
      }
      h1 {
        font-weight: normal;
        font-size: 55px;
      }
      h2 {
          font-family: 'Bitter';
      }
      li {
        margin-bottom: 15px;
      }
      ul {
          margin: 0;
          margin-top: 10px;
      }
      a {
        color: #eaab0c;
        text-decoration: 1px dotted #ccc;
      }
      p {
          margin: 0px;
      }
      .slide {
          
      }
      iframe {
        position: relative;
        width: 750px;
        height: 450px;
        margin: 0 auto;
      }
      .center {
        margin: 0 auto;
      }
      .footnote {
        position: absolute;
        bottom: 3em;
      }

      .refs {
        font-size: 18px;
      }

      .red    { color: #FF4943; }
      .gray   { color: #787878; }
      .green  { color: #92d12d; }
      .blue   { color: #41C8F0; }
      .yellow { color: #dbcb14; }
      .orange { color: #eaab0c; }

      
      .people {
          border-top: 1px solid #f1f1f1;
      }
      

      .task {
        /* float: right; */
        position: absolute;
        top: 25px;
        right: 50px;
        font-size: 18px;
        padding-top: 0.6em;
        color: #d3d3d3;
      }
      .task a {
        color: #080;
        text-decoration: none;
      }      

      .footnote {
        position: absolute;
        bottom: 3em;
      }
      
      .shortlist {
          font-size: 27px;
      }
     
      
      .rotatetitle {
          transform:rotate(-12deg);
          -ms-transform:rotate(12deg);
          -moz-transform:rotate(12deg);
          -webkit-transform:rotate(-12deg);
          -o-transform:rotate(-12deg);

          -ms-transform-origin:150% -170%;
          -moz-transform-origin:150% -170%;
          -webkit-transform-origin:150% -170%;
          -o-transform-origin:150% -170%;

          font-size: 15px;
      }
      
      .smallquote {
          font-size: 14px;
      }
      
      .position {
          color: #ccc;
          font-size: 12px;
      }
      
      pre code {
        -webkit-border-radius: 6px;
        -moz-border-radius: 6px;
        border-radius: 6px;
      }


      /* Two-column layout */
      .left-column {
        width: 70%;
        float: left;
      }

      .right-column {
        width: 30%;
        float: right;
      }
      
      #slideshow .slide .content code { font-size: 11px;
        
      }
   </style>
  </head>
  <body>
    <textarea id="source">

name: title
class: center, middle

# Using .blue[Mechanical Turk] and .blue[PsiTurk] for Dynamic Web Experiments
.author[
Anna Coenen ([@AnnaCoenen](http://twitter.com/AnnaCoenen)), Doug Markant ([@dougmarkant](http://twitter.com/dougmarkant)), Jay Martin, and John McDonnell ([@johnvmcdonnell](http://twitter.com/johnvmcdonnell))  
_[Computation and Cognition Lab](http://gureckislab.org)_  
_Department of Psychology_  
_New York University_  
]


---

name: goals
class: shortlist, middle

# Our three goals

1. Give an introduction to Amazon Mechanical Turk

2. Discuss the capabilities and limits of web-based experiments

3. Introduce the psiturk toolbox for making AMT-based web experiments as painless as possible

(Get slides [here](https://github.com/NYUCCL/MTurkWorkshop_CogSci2013))
---

name: whatisamt
class: center, middle
# What is ".blue[Mechanical Turk]"?

---

.task[Part 1: What is AMT?]
# The history


- Developed by [Amazon.com](http://amazon.com)

- Originally for in-house use to detect duplicate product postings on Amazon's site .red[*]

<br><br>
.center[<img src="images/AMTLogo.png" width="300">
]



.footnote[.red[*] Nice summary in the [New York Times](http://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html)]


---


name: terminology
class: shortlist, middle

.task[Part 1: What is AMT?]
# Key terminology


- HIT = Human Intelligence Task (a unit of work, e.g. a trial or an entire sequence of trials in an experiment)

- .orange[**Requester**] = an entity (e.g., researcher) who posts HITs

- .blue[**Worker**] = a person who performs the task

---


.task[Part 1: What is AMT?]
# Who are the workers?

- 46.80% US, 34% India, 19.20% Other

- United States demographic
    - 55-65% female
    - Most make  <$60k/year
    - Median age of 30
    - Hold bachelor's and are young
    - Distribution mostly similar to US internet pop.
    
- See Ipeirotis, et al. (2010) or Mason and Siddharth (2011).

---

.task[Part 1: What is AMT?]
# World Distribution

<img src="images/world-turk-distribution.jpg" width="700">

Tamir (2011)


---

.task[Part 1: What is AMT?]
# Compensation

- Median wage is $1.38/hour

- Short tasks (~5 mins) award around 10 cents

- Requester can reject work and revoke payment

- And can also **award bonuses**

- Amazon takes 10% of payments

- Amazon tries to stay out of disputes


---


.task[Part 1: What is AMT?]
# Outside of the US

- Unfortunately, getting a requester account from outside the US can be a bit tricky

- Requires US billing address  and US debit or credit card

- If you can't get a US credit card, setting up the account with a US American collaborator is an option

- Or consider international/European alternatives to AMT, such as [clickworker](http://www.clickworker.com), [crowdflower](http://crowdflower.com). The latter actually uses AMT workers, but is accessible worldwide 

- (the psiTurk toolbox could in principle be adapted to work with other platforms)


---

.task[Part 1: What is AMT?]
# Why use web experiments?

- **Convenient**
    - And the psiTurk toolbox will help you even more
- **Fast** 
    - collect a lot of data quickly, also good for piloting
- **Affordable**
    - Workers will sign up for $2.00 for 15-25 minute session
- **Anonymous**
    - Subject never meets experimenter
    - Subjects unlikely to know about "your lab"
- **Replicable**
    - Very easy to share your experiment code


---


.task[Part 1: What is AMT?]
# Why not?
A few concerns have been raised abut AMT data:
<br><br>

- **Selection Bias:** No control over who takes the experiment
    - Makes AMT sample probably less useful for research on ideology, or political attitudes (53% are self-identified liberals, 25% conservatives, 73% voted for Obama)

    - See [this](http://www.culturalcognition.net/blog/2013/7/10/fooled-twice-shame-on-who-problems-with-mechanical-turk-stud.html) blogpost for a discussion

    - On the other hand, 20 year old university students are a very "special" sample too!

---
.task[Part 1: What is AMT?]
# Why not?

- **Contamination of subject base** 
    - Recent [study](http://www.jessechandler.com/uploads/2/8/0/5/2805897/13_chandler_mueller__paolacci.pdf) has addressed this question
    - Can be a problem for *very* widely used paradigms 
      - 56% had participated in Prisoner's Dilemma, 52% in Ultimatum game, 30% Trolley problem

    - Cross talk (e.g. in AMT forums) seems to be rare (workers share information about payment, duration, etc.)

    - For specific studies repeated participation can be prevented by recording worker's IDs
 - .blue[PsiTurk toolbox helps with this!]

---
.task[Part 1: What is AMT?]
# Why not?
- **Non-lab setting **
    - [This](http://www.jessechandler.com/uploads/2/8/0/5/2805897/13_chandler_mueller__paolacci.pdf) study showed
      - 27% were not alone while working on the HIT
      - 18% were watching tv
      - 14% were listening to music

    - Recording how often participants change windows/take breaks might help clean data
  - .blue[PsiTurk toolbox helps with this!]

    - Still, the lack of experimental control over a worker's environment might make some studies unsuitable for AMT


---


class: center, middle

# How does the data compare to that collected in the lab?


---

name: replicationstudies

.task[Part 1: What is AMT?]
# Lots of interest in this...

.refs[

- Gosling, S.D., Vazire, S., Srivastava, S., & John, O.P. (2004). [Should we trust web-based studies? A comparative analysis of six preconceptions about Internet questionnaires](http://ww.w.simine.com/docs/Gosling_et_al_AP_2004.pdf). _American Psychologist_, 59, 2, 93-104.

- Paolacci, G., Chandler, J., & Ipeirotis, P. G. (2010). [Running experiments on Amazon Mechanical Turk](http://repub.eur.nl/res/pub/31983/jdm10630a[1].pdf). _Judgment and Decision Making_, 5, 411-419.

- Buhrmester, M., Kwang, T., & Gosling, S. D. (2011). [Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data?](http://pps.sagepub.com/content/6/1/3.full). _Perspectives on Psychological Science_, 6(1), 3-5.

- Germine, L., Nakayama, K., Duchaine, B.C., Chabris, C.F., Chatterjee, G. & Wilmer, J.B. (2012).  [Is the Web as good as the lab?  Comparable performance from Web and lab in cognitive/perceptual experiments](http://www.springerlink.com/content/f0244t772070138w/)  _Psychonomic Bulletin & Review_,  19.5.


- Shapiro, D. N., Chandler, J., & Mueller, P. A. (2013). [Using Mechanical Turk to Study Clinical Populations](http://s3.amazonaws.com/academia.edu.documents/30554524/Clinical_Psychological_Science-2013-Shapiro-2167702612469015.pdf?AWSAccessKeyId=AKIAIR6FSIMDFXPEERSA&Expires=1374090987&Signature=%2B4nErhKWOQhoWYY9gpgV0EbvVa0%3D&response-content-disposition=inline). _Clinical Psychological Science_, 1(2), 213-220.

]

---


.task[Part 1: What is AMT?]
# Do classic findings replicate?

- Some members of our lab recently contributed to this study that tested some classic psychology experiments on AMT.red[*]

    - Crump, M. J., McDonnell, J. V., & Gureckis, T. M. (2013). [Evaluating Amazon's Mechanical Turk as a tool for experimental behavioral research.](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0057410) PloS one, 8(3).

- The goal was to see if classic findings replicate and also check what methods might help increase the quality of the data

- Focus was on reaction time findings that require sustained attention from subjects and precise recording of responses

- And learning experiments involving more cognitive effort 


.footnote[.red[*] For a summary of the paper see this [blog post](http://gureckislab.org/blog/?p=1297)]

---



.task[Part 1: What is AMT?]
# Presentation Time
.left-column[ 

Stroop

  - People respond faster to congruent (e.g. .blue[blue]) than to incongruent (.green[blue]) items


Task Switching

  - Faster RT if task stays the same (e.g. "Odd or Even?") than when tasks alternate ("Odd or Even?" / "Small or Large?")

Flanker

  - When identifying a target (e.g. "h"), congruent flankers ("hhhhh") lead to faster RTs than incongruent ("ffhhff") ones

]
  

.center[.right-column[
<img src="images/StroopResult.png" width="160">
<img src="images/TaskSwitchingResult.png" width="160">
<img src="images/FlankerResult.png" width="160">
]]  


---

.task[Part 1: What is AMT?]
# Presentation Time
.left-column[ 

Simon

  - Targets that are spatially compatible with response key are identified faster


Visual Cuing 

  - Stimulus detection is faster for cued  versus uncued targets if cue is presented after short interval (~300ms) and slower after longer interval (~400ms)

Attentional Blink

  - During rapid serial visual presentation visual target detection is impaired if target is displayed 100-500ms after another one


]


.center[.right-column[
<img src="images/SimonResult.png" width="160">
<img src="images/ContCueing.png" width="160">
<img src="images/AttentionalBlink.png" width="160">
]]  

---


.task[Part 1: What is AMT?]
# Subliminal Perception

Masked priming

  - Responding to arrow probes that are either compatibly (prime: >>, probe: >>) or incompatibly primed (prime: <<, probe: >>) at durations of 16, 32, 48, 64, 80, and 96 ms 

  -  Previous finding was that compatibility effects are negative for very short prime durations and positive for longer ones


  .center[
   <img src="images/PrimingResult.png" width="330">
   ]

  - Only partially replicated: no negative compatibility effect

  - Possibility: Lower bound on display duration in browser?

---


.task[Part 1: What is AMT?]
# Category Learning

- Classic Shepard, Hovland, & Jenkins (1961) paper

- Six different ways of classifying the same 8 items (geometric figures with 3 binary dimensions) into deterministic categories

- Generally Type I is learned faster than Type II which is faster that Types III-V, with Type VI the hardest
.center[
<img src="images/abstractstructureSHJ.png" width="150"><img src="images/dimstructureSHJ.png" width="170">
<img src="images/SHJDifficulty.png" width="550">
]

---

.task[Part 1: What is AMT?]
# Results

Although type I was learned easily, performance on all other types was significantly worse than in the original study
<br><br>

.center[
<img src="images/exp8-nosofsky.png" height="290"><img src="images/exp8-amt.png" height="290">
]


---




.task[Part 1: What is AMT?]
# Incentives too low?
There was little difference between high ($2 to $4.50 with bonus), medium ($1.50), and low-payment ($.75) groups
<br><br>

<img src="images/exp9-typeII.png" height="290">
<img src="images/exp9-typeIV.png" height="290">


---


.task[Part 1: What is AMT?]
# Bad instructions?
Including manipulation checks that asked non-trivial questions about the task improved performance on types II to VI
<br><br>

<img src="images/exp10-n-vs-amt.png" height="310">
<img src="images/exp10-l-vs-amt.png" height="310">

---


.task[Part 1: What is AMT?]
# Summary

- Important outcomes
    - Most findings replicated with high fidelity

    - But experiments that rely on very precise and fast stimulus presentation may reach the limit of most browsers

    - Payment didn't have much effect on performance but did affect signup and dropout rates

    - Checking participants' understanding of the task increased data quality considerably
    
<!-- - Note: dropout rates are much higher than in the lab, so maybe consider reporting those if they might interact with performance
 -->


---

.task[Part 1: What is AMT?]
# Replication in other Areas

1. Clinical (Shapiro, et al, 2013)

  - Scores on a range of psychometric tests had high internal consistency and test-retest reliability  (r =.87)

2. Personality (Buhrmeser, et al, 2011)

  - No difference in consistency on personality questionnaires at different payment levels, and absolute levels of consistency and test-retest reliability were high (r = .88)

3. JDM (Paolacci, et al, 2010)
  
  -  Compared classic JDM studies (Asian Disease, Linda, Physicians Problem) on AMT and university subject pool
  - No difference in failure rate of catch trials and pattern of behavior in studies but AMT subjects were more risk averse

---

.task[Part 1: What is AMT?]
# A word on Ethics

- How much should you pay?  
    - As much as the market supports or should it match what you pay in the lab (roughly)?
    - AMT has been criticized for incentivizing companies to outsource labor without adequate compensation (for discussions see for example [here](http://priceonomics.com/who-makes-below-minimum-wage-in-the-mechanical/?utm_source=buffer&utm_campaign=Buffer&utm_content=buffer2d486&utm_medium=twitter), [here](http://economix.blogs.nytimes.com/2013/03/18/the-unregulated-work-of-mechanical-turk/), and [here](http://www.huffingtonpost.com/julian-dobson/mechanical-turk-amazons-underclass_b_2687431.html))

- Can you get IRB approval for your experiment?

    - Arguably, workers have more freedom of choice than lab participants (can stop experiment any time)

    - Wasn't very difficult to add to an existing IRB at NYU


---

.task[Part 1: What is AMT?]
# Be a good requester
 
Workers share information about good and bad requesters:
<br><br>
.center[
<img src="images/Turkopticon.png" height="400">
]
 
---

.task[Part 1: What is AMT?]
# Be a good requester
 
A few recommendations to keep workers happy...

1. Before running, test your experiments on .blue[different browsers] and the sandbox (AMT's testing environment)

1. Give explicit instructions and .blue[show warnings] about closing the page, going back in the browsers, etc.

1. Start collecting data in .blue[small batches]
 
1. If something goes wrong, provide an .blue[alternative payment method] for workers who are unable to finish or submit HIT
    - One way of doing this is to provide a HIT with no content specifically aimed at workers that had problems and pay them using a "bonus"
 

---

class: center, middle

.task[Part 1: What is AMT?]

# End of .blue[Part 1]
Questions?



---


name: part2
class: center, middle
 
.task[Part 2: Demo]
 
# Using AMT and the psiTurk Dashboard


---


class: middle
.task[Part 2: Using AMT]

# Two perspectives:
 
1. What is it like to use AMT as a [**Worker**](#terminology)?
 
2. What is it like to use AMT as a [**Requester**](#terminology)?

# Two platforms:

1. The sandbox

    - Worker sandbox ([workersandbox.mturk.com](http://workersandbox.mturk.com))

    - Requester sandbox ([requestersandbox.mturk.com](http://requestersandbox.mturk.com))

2. The real deal ([www.mturk.com](http://www.mturk.com))


---


.task[Part 2: Using AMT]
# What is a HIT ?


- A unit of work, or a "Human Intelligence Task"

- In our experiments, a HIT is typically an entire experiment (as might be done in the lab)
 
- However, it is up to the .orange[**Requester**] to determine what constitutes a HIT
 
- Could be one trial in an experiment, and you just offer the .blue[**Worker**] as many HITs as they care to do
 

---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**]
 
1. Sign up on Amazon's site
 
2. Search for tasks (i.e., HITs) to do online through the AMT interface.
 
3. Worker accepts a HIT.  The task appears in a frame within the AMT website.

    - Accepting a HIT means it is removed from the .orange[**Requester**]'s account. If the Worker decides not to complete the HIT, they have to _return_ it to the .orange[**Requester**] so someone else can do it

4. After completing the HIT, the worker signals to Amazon that they have completed the task
 
5. Payment will be credited to a worker's account some time after completion


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Search view


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Hit details 


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Ad view


---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
1. Open a Amazon Web Services (AWS) account: [Subscribe here](http://aws.amazon.com)
 
1. Get a Mechanical Turk Requester Account: [Subscribe here](http://requester.mturk.com)
   
1. Create a "dummy" email account which you can use to deal with inevitable issues that come up with Workers
    - Workers ***will*** contact you when they have issues
    - Need a way to deal with this efficiently and quickly (your reputation depends on it!)
     
1. Use the built-in Amazon template or "External Question" approach to design your experiment
 
1. Test your code on the "sandbox" website before going live
 
1. Go live! 
 
 
---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
## Creating a project

 
---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
## Creating a new batch


---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]

## Crediting participants


---

 
.task[Part 2: Using AMT]
# AMT's built-in question templates

###Why use them?
- Hosted directly on Amazon's servers
- Good for simple surveys (multiple choice, text fields, radio buttons)
- Easy to use (don't need your own server)
- Tools for helping you "build" the question
- Basic HTML can be included (to format texts, or load data, images, etc. from a separate datafile, for example)
- HITs can be individualized with different content for each


---

.task[Part 2: Using AMT]
# AMT's built-in question templates

###But...
- Dynamic elements cannot be included!
- For those purposes, you want to use AMTs **external question** type


---


.task[Part 2: The mechanics]
# External questions 
 
- You are responsible for:
    - Hosting the experiment
       - **At minimum** you need a computer with a web-accessible HTTP server running (a static IP address may simplify things)
       - You probably want to avoid trying to host it over WiFi 

    - Posting your link to Amazon
     
    - Communicating back to Amazon when the HIT has been completed
     
    - Approving completed HITs for payment in a timely manner
     
    - Dealing quickly with .blue[**Worker**] issues
     
    - Saving your data in a secure place, etc..


---


.task[Part 2: Using AMT]
# External questions

- Interactions with AMT that psiTurk handles:

    - Submit a request for HITs (through the Dashboard)

    - Provide a link to an "ad" that describes the HIT for Workers (this appears within the AMT site when they click 'View a HIT in this group') .red[[1]]

    - After HIT is accepted, provide a link to the experiment .red[[1]]

    - After HIT is complete, provide a button Worker will click to inform Amazon that they have finished. This submits a form with their identifying information to AMT. .red[[2]]


.footnote[
psiTurk hint .red[[1]]: see templates/mturkindex.html<br />
psiTurk hint .red[[2]]: see templates/thanks.html
]


---


.task[Part 2: Using AMT]
# External questions

- When participants see your HIT in the list, they will see basic metadata:

<img src="images/listing.png" width="750" />

- Once they click "View a HIT in this group," they see whatever your site is serving up, in our case this is the default:


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_dashboard.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_awsinfo.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_database.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_server.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_hitconfig.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_expinfo.png" width="750" />


---


.task[Part 2: Using AMT]
# External questions

## A Participant Accepts

Upon accepting the HIT, they see... whatever you want them to see! In our case,
it's this:

The link to the site. Could be anything! Must link back to Amazon at the end (we'll show you how)


---


.task[Part 2: Using AMT]
# External questions

The last thing served to the participant is a button they can use to inform
Amazon that they have finished the HIT. This submits a form with their
identifying information to `http://www.mturk.com/mturk/externalSubmit`.


---


.task[Part 2: Using AMT]
# External questions

Error upon trying again.


---

.task[Part 2. Using AMT]
# Be a good requester
 
1. Test your experiment on .blue[different browsers]

1. Use the .blue[sandbox] for testing!

1. Provide users with explicit instructions and .blue[show warnings] about closing the page, going back in the browsers, etc.
    - For many more "basic" HITs it's ok to close the window and return another time

1. Start collecting data in .blue[small batches]
 
1. If something goes wrong, provide an .blue[alternative payment method] for workers who did your experiment but were unable to submit the HIT
    - One way of doing this is to provide a HIT with no content specifically aimed at workers that had problems and pay them using a "bonus"


---


class: center, middle
# End of .blue[Part 2]
### .gray[Time for a break!]
 

---

name: part3
class: center, middle

.task[Part 3: Building dynamic web experiments]

# .blue[Part 3]: Building dynamic web experiments


---

.task[Part 3: Building dynamic web experiments]
# Programming for the web:<br> .blue[The Good]

- Take advantage of all the perks of online experiments (as discussed in part 1)
- Web experiments will run in the lab too!
- There is a vast amount of open source software available for customizing websites for your specific needs and a large community to help out

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: <br>.blue[The Challenge]

To get your experiment up and running on AMT you need to:

- Host it on your own server
- Post the advertisement and request for subjects to Amazon's servers.
- Communicate with Amazon when a HIT has been completed
- Save the data 
- Become a web developer

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: Luckily...

The psiTurk toolbox takes care of most of this! 

---

class: center, middle

.task[Part 3: Building dynamic web experiments]
# psiTurk
### .gray[Our framework for doing online experiments]
### [https://github.com/NYUCCL/psiTurk](https://github.com/NYUCCL/psiTurk)
<img src="images/psiturk_logo_cloud_2l.png" width="300">
<img src="images/github.png" width="200">


---

.task[Part 3: Building dynamic web experiments]
# What does psiTurk do for you?

- Runs a .blue[**webserver**] to host your experiment

- Provides a .blue[**local database**] to 
    - Store data incrementally while a worker completes a HIT
    - Allow you to verify that workers have not participated in your experiment before

- Handles your .blue[**interactions with Amazon**], including...
    
    - Managing your HITs
    - Launching experiments as external HIT
    - Communicating back to AMT when job is done
    - Facilitates paying workers after completing a HIT

- Provides a browser based .blue[**user-interface**] (dashboard) that lets you do all of this without the need to use the command line

- Includes a library of .blue[**useful Javascript functions**] for structuring your experiment, as well as a simple .blue[**example experiment**] that can be used as a template

.left[*NB: Except for the Amazon functionalities, all other parts of psiturk are useful for deploying web experiments in general!*]


---

.task[Part 3: Building dynamic web experiments]
# What does it NOT for you?

- Even though psiTurk provides code examples and useful functions, you will still need to learn some basic web programming to design your experiment

- That requires some knowledge of at least the following:
    - .blue[**HTML**] 
    - .blue[**CSS**]
    - .blue[**Javascript**], and .blue[**JQuery**] (a JS library)

- Luckily, there exist a LOT of resources for learning to program for the web, e.g.
    - Codeacademy turorials on [Web Fundamentals](http://www.codecademy.com/tracks/web) (HTML and CSS), [JavaScript](http://www.codecademy.com/tracks/javascript), and [JQuery](http://www.codecademy.com/tracks/jquery)
    - [Introduction to Web development](https://developer.mozilla.org/en-US/docs/Web_Development/Introduction_to_Web_development) resources from the Mozilla  Developer Network
    - The [Stackoverflow](http://stackoverflow.com) community will have answers to 99% of amateur web programming questions


This may seem overwhelming at first, but after a steep learning curve, blah blah, some motivational statement...


---


.task[Part 3: Building dynamic web experiments]
# JavaScript

**What?**  

- Most likely, JavaScript will be the main work horse of your web experiment
- It is a language that browsers can understand to make websites dynamic
- It will control the logic of your task, handle communication with the server, etc.


**Can Dos (for cognitive experiments)**  

- Interact with a web server (to send or request data for instance)
- Track clicks, mouse movements, etc.
- Record response times
- Track changes in focus and window size
- Enable group experiments (via web sockets)


**Can't Dos**  

- Control focus (lock participants in a window)
- Disable browser actions (reloading, going back)
- Saving directly to a file (server interaction is required)


---

.task[Part 3: Building dynamic web experiments]
# PsiTurk JS functions

To help you get started to build your experiment we included a small library that will help you do the following:

- Handle page transition
- Load images
- Register if user resizes their Window
- Register if user switches to another tab/window
- Record and save data

---

.task[Part 3: Building dynamic web experiments]
# The Psiturk javascript object

Some of the important Javascript functions we provide:

```javascript
exp = new Psiturk()

// Handling experiment info: 
exp.taskdata.get("condition") // returns subect's server-assigned condition
exp.taskdata.get("counterbalance") // returns subect's server-assigned condition

// Handling resources:
exp.getPage("mypage.html") // Returns HTML from mypage.html, stored in the templates directory

// Recording subjects' responses
exp.recordTrialData([1, "L", "CORRECT"]) // Records data for the current trial
exp.recordUnstructuredData("Age", 28) // Records data for the current trial

// Save data to server
exp.saveData()
```

---

.task[Part 3: Building dynamic web experiments]
# Javascript libraries

- .blue[**Jquery**] -- Manipulating the website
- .blue[**BackboneJS**] -- Organizing your code
- .blue[**Underscore**] -- Helpful functions, e.g. `_.shuffle()` to shuffle stimuli

<!-- ---

(ANNA: Not sure if this should even go here. Or how we can elaborate on this. It seems good to have it in the section that says what psiturk can do, but it's perhaps a bit strange to do this before we even describe what it is...)


.task[Part 4: Using psiTurk]
# What is it actually?

- A small webserver that you run in Python that makes it very easy to customize

- It runs on a designated port on your computer

- You point the Workers at this url

- Based on three key open-source packages:

<img src="http://flask.pocoo.org/static/logo/flask.png" width="250">  
<img src="http://www.sqlalchemy.org/img/sqla-logo6.gif" width="250">  
[boto](https://github.com/boto/boto) -->

---


name: part4
class: center, middle

.task[Part 3: Using psiTurk]
# .blue[Part 4]: Using psiTurk

---


.task[Part 4: Using psiTurk]
# Installation

**1. System requirements **    

- psiTurk works only on OS X and Linux, Windows is not supported (cloud-based option coming soon!).

<br>
**2. Software dependencies**    

- **Python**

  - If you've never used Python before, we recommend you use the [Enthought Python distribution](http://www.enthought.com/repo/.epd_academic_installers), 
  because it comes with pip, a tool for easily installing Python packages. 

  <br>
  <br>

---


.task[Part 4: Using psiTurk]
# Installation Con't


**2. Software dependencies con't**    

- **pip**

  - If you installed Python using Enthought you don't need to install pip separately. If you want to use another version of Python, you may have to install pip, for example like this:


    .bash
    curl -O https://raw.github.com/pypa/pip/master/contrib/get-pip.py
    [sudo] python get-pip.py



**3. Installing psiTurk**

- To install psiTurk simply run:


    .bash
    pip install git+git://github.com/NYUCCL/psiTurk.git@dev

    (should be on master branch by workshop)

---



.task[Part 4: Using psiTurk]

# Running the Stroop demo

***Goal***

- Post a HIT on AMT.


**1. Sign up for [AWS account](http://aws.amazon.com/) **

**2. Check out the demo repository**

    .bash
    git clone git://github.com/NYUCCL/psiTurk.git

- or download it as a [zip file](https://github.com/NYUCCL/psiTurk/archive/master.zip).
- You can modify this experiment to create your own.


**3. Move into the repository**

    .bash
    cd psiturk


**4.Start the dashboard**

    .bash
    psiturk


---


.task[Part 4: Using psiTurk]

# Modifying the Stroop task

***Goal***

- Use our demo code as the base for a new project


** Directory structure **

- psiTurk expects to find files in certain places
- You'll spend most of your time in the static folder
- /static
    - task.js (logic)
    - task.css (appearance)


** Let's get our hands dirty...**


---



class: middle, center

# Thanks!


    </textarea>
    <script src="remark.min.js" type="text/javascript">
        
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({ "highlightStyle": "tomorrow-night-bright" });
    </script>
  </body>
</html>
