<!DOCTYPE html>
<html>
  <head>
    <title>Using Mechanical Turk for Cognition Experiments - John McDonnell and Todd Gureckis</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css" media="screen">
    /* Slideshow styles */
      @import url(http://fonts.googleapis.com/css?family=Lobster);
      @import url(http://fonts.googleapis.com/css?family=Oxygen);
      
      body { 
        font-family: 'Oxygen', sans-serif;
        font-size: 24px;
      }
      h1 {
          font-family: 'Lobster', cursive;
          color: #444;
      }
      h1 {
        font-weight: normal;
        font-size: 55px;
      }
      h2 {
          font-family: 'Bitter';
      }
      li {
        margin-bottom: 15px;
      }
      ul {
          margin: 0;
          margin-top: 10px;
      }
      a {
        color: #eaab0c;
        text-decoration: 1px dotted #ccc;
      }
      p {
          margin: 0px;
      }
      .slide {
          
      }
      iframe {
        position: relative;
        width: 750px;
        height: 450px;
        margin: 0 auto;
      }
      .center {
        margin: 0 auto;
      }
      .footnote {
        position: absolute;
        bottom: 3em;
      }

      .refs {
        font-size: 18px;
      }

      .red    { color: #FF4943; }
      .gray   { color: #787878; }
      .green  { color: #92d12d; }
      .blue   { color: #41C8F0; }
      .yellow { color: #dbcb14; }
      .orange { color: #eaab0c; }

      
      .people {
          border-top: 1px solid #f1f1f1;
      }
      

      .task {
        /* float: right; */
        position: absolute;
        top: 25px;
        right: 50px;
        font-size: 18px;
        padding-top: 0.6em;
        color: #d3d3d3;
      }
      .task a {
        color: #080;
        text-decoration: none;
      }      

      .footnote {
        position: absolute;
        bottom: 3em;
      }
      
      .shortlist {
          font-size: 27px;
      }
     
      
      .rotatetitle {
          transform:rotate(-12deg);
          -ms-transform:rotate(12deg);
          -moz-transform:rotate(12deg);
          -webkit-transform:rotate(-12deg);
          -o-transform:rotate(-12deg);

          -ms-transform-origin:150% -170%;
          -moz-transform-origin:150% -170%;
          -webkit-transform-origin:150% -170%;
          -o-transform-origin:150% -170%;

          font-size: 15px;
      }
      
      .smallquote {
          font-size: 14px;
      }
      
      .position {
          color: #ccc;
          font-size: 12px;
      }
      
      pre code {
        -webkit-border-radius: 6px;
        -moz-border-radius: 6px;
        border-radius: 6px;
      }


      /* Two-column layout */
      .left-column {
        width: 70%;
        float: left;
      }

      .right-column {
        width: 30%;
        float: right;
      }
      
      #slideshow .slide .content code { font-size: 11px;
        
      }
   </style>
  </head>
  <body>
    <textarea id="source">

name: title
class: center, middle

# Using .blue[Mechanical Turk] and .blue[PsiTurk] for Dynamic Web Experiments
.author[
Anna Coenen ([@AnnaCoenen](http://twitter.com/AnnaCoenen)), Doug Markant ([@dougmarkant](http://twitter.com/dougmarkant)), Jay Martin, and John McDonnell ([@johnvmcdonnell](http://twitter.com/johnvmcdonnell))  
_[Computation and Cognition Lab](http://gureckislab.org)_  
_Department of Psychology_  
_New York University_  
]

---

name: overview

# Overview

1. What is [Amazon Mechanical Turk](https://www.mturk.com) (AMT)?  [Jump to this section](#whatisamt)
    - Who are the people in the system (demographics, etc...)?
    - How good is the data?

2. Running dynamic experiments on AMT [Jump to this section](#part2)

    - Show how to use the AMT website 
    - Step through setting up an experiment using psiTurk 
    
3. Basics of running experiments in the lab [Jump to this section](#part3)

    - Introduce a [simple Python-based web-app](https://github.com/NYUCCL/psiTurk) we developed which could be a starting framework for your experiment
    - Using the AMT sandbox to develop and test your app
    - Manage hits, using a browser-based dashboard 


---

name: goals
class: shortlist, middle

# Our three goals

1. Give an introduction to Amazon Mechanical Turk

2. Discuss the capabilities and limits of web-based experiments

3. Introduce the psiturk toolbox for making AMT-based web experiments as painless as possible


---

name: whatisamt
class: center, middle
# What is ".blue[Mechanical Turk]"?

---

.task[Part 1: What is AMT?]
# The history

<img src="images/bezos.jpg" width="250" align="right"><br>


- Developed by [Amazon.com](http://amazon.com) by Peter Cohen

- Originally for in-house use to detect duplicate product postings on Amazon's site .red[*]

- Jeff Bezos calls it "artificial artificial intelligence"

- Name originates from a mechanical chess illusion/automaton by Wolfgang von Kempelen in the 18th century


.footnote[.red[*] Nice summary in the [New York Times](http://www.nytimes.com/2007/03/25/business/yourmoney/25Stream.html)]


---


name: terminology
class: shortlist, middle

.task[Part 1: What is AMT?]
# Key terminology


- HIT = Human Intelligence Task (a unit of work, e.g. a trial or an entire sequence of trials in an experiment)

- .orange[**Requester**] = an entity (e.g., researcher) who posts HITs

- .blue[**Worker**] = a person who performs the task

---


.task[Part 1: What is AMT?]
# Who are the workers?

- 46.80% US, 34% India, 19.20% Other

- United States demographic
    - 55-65% female
    - Most make  <$60k/year
    - Median age of 30
    - Hold bachelor's and are young
    - Distribution mostly similar to US internet pop.
    
- See Ipeirotis, et al. (2010) or Mason and Siddharth (2011).

---

.task[Part 1: What is AMT?]
# US Distribution

<img src="images/us-turk-distribution.jpg" width="700">

Taken from this [website](http://www-958.ibm.com/software/data/cognos/manyeyes/visualizations/geographic-distribution-of-turkers).

---

.task[Part 1: What is AMT?]
# World Distribution

<img src="images/world-turk-distribution.jpg" width="700">

Tamir (2011)


---

.task[Part 1: What is AMT?]
# Compensation

- Median wage is $1.38/hour

- Short tasks (~5 mins) award around 10 cents

- Requester can reject work and revoke payment

- And can also **award bonuses**

- Amazon takes 10% of payments

- Amazon tries to stay out of disputes


---


.task[Part 1: What is AMT?]
# Outside of the US

- Unfortunately, getting a requester account from outside the US can be a bit tricky

- Requires US billing address  and US debit or credit card

- If you can't get a US credit card, setting up the account with a US American collaborator is an option

- Or consider international/European alternatives to AMT, such as [clickworker](http://www.clickworker.com), [crowdflower](http://crowdflower.com). The latter actually uses AMT workers, but is accessible worldwide 

- (the psiTurk toolbox could in principle be adapted to work with other platforms)


---

.task[Part 1: What is AMT?]
# Why use web experiments?

- **Convenient**
    - And the psiTurk toolbox will help you even more
- **Fast** 
    - collect a lot of data quickly, also good for piloting
- **Affordable**
    - Workers will sign up for $2.00 for 15-25 minute session
- **Anonymous**
    - Subject never meets experimenter
    - Subjects unlikely to know about "your lab"
- **Replicable**
    - Very easy to share your experiment code


---


.task[Part 1: What is AMT?]
# Why not?
A few concerns have been raised abut AMT data:
<br><br>

- **Selection Bias:** No control over who takes the experiment
    - Makes AMT sample probably less useful for research on ideology, or political attitudes (53% are self-identified liberals, 25% conservatives, 73% voted for Obama)

    - See [this](http://www.culturalcognition.net/blog/2013/7/10/fooled-twice-shame-on-who-problems-with-mechanical-turk-stud.html) blogpost for a discussion

    - On the other hand, 20 year old university students are a very "special" sample too!

---
.task[Part 1: What is AMT?]
# Why not?

- **Contamination of subject base** 
    - Recent [study](http://www.jessechandler.com/uploads/2/8/0/5/2805897/13_chandler_mueller__paolacci.pdf) has addressed this question
    - Can be a problem for *very* widely used paradigms 
      - 56% had participated in Prisoner's Dilemma, 52% in Ultimatum game, 30% Trolley problem

    - Cross talk (e.g. in AMT forums) seems to be rare (workers share information about payment, duration, etc.)

    - For specific studies repeated participation can be prevented by recording worker's IDs (see psiTurk sections)


---
.task[Part 1: What is AMT?]
# Why not?
- **Non-lab setting **
    - [This](http://www.jessechandler.com/uploads/2/8/0/5/2805897/13_chandler_mueller__paolacci.pdf) study showed
      - 27% were not alone while working on the HIT
      - 18% were watching tv
      - 14% were listening to music
      - 55% were monitoring other requesters at the same time

    - Recording how often participants change windows/take breaks might help clean data (again, see psiTurk section)

    - Still, the lack of experimental control over a worker's environment might make some studies unsuitable for AMT


---


class: center, middle

# How does the data compare to that collected in the lab? .red[*]


.left.footnote[.red[*]Check out our discussion of this at our [Thinking about Thinking](http://gureckislab.org/blog/) blog ]


---

name: replicationstudies

.task[Part 1: What is AMT?]
# Lots of interest in this...

.refs[

- Gosling, S.D., Vazire, S., Srivastava, S., & John, O.P. (2004). [Should we trust web-based studies? A comparative analysis of six preconceptions about Internet questionnaires](http://ww.w.simine.com/docs/Gosling_et_al_AP_2004.pdf). _American Psychologist_, 59, 2, 93-104.

- Paolacci, G., Chandler, J., & Ipeirotis, P. G. (2010). [Running experiments on Amazon Mechanical Turk](http://repub.eur.nl/res/pub/31983/jdm10630a[1].pdf). _Judgment and Decision Making_, 5, 411-419.

- Buhrmester, M., Kwang, T., & Gosling, S. D. (2011). [Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data?](http://pps.sagepub.com/content/6/1/3.full). _Perspectives on Psychological Science_, 6(1), 3-5.

- Germine, L., Nakayama, K., Duchaine, B.C., Chabris, C.F., Chatterjee, G. & Wilmer, J.B. (2012).  [Is the Web as good as the lab?  Comparable performance from Web and lab in cognitive/perceptual experiments](http://www.springerlink.com/content/f0244t772070138w/)  _Psychonomic Bulletin & Review_,  19.5.


- Shapiro, D. N., Chandler, J., & Mueller, P. A. (2013). [Using Mechanical Turk to Study Clinical Populations](http://s3.amazonaws.com/academia.edu.documents/30554524/Clinical_Psychological_Science-2013-Shapiro-2167702612469015.pdf?AWSAccessKeyId=AKIAIR6FSIMDFXPEERSA&Expires=1374090987&Signature=%2B4nErhKWOQhoWYY9gpgV0EbvVa0%3D&response-content-disposition=inline). _Clinical Psychological Science_, 1(2), 213-220.

]

---


.task[Part 1: What is AMT?]
# Do classic findings replicate?

- Some members of our lab recently contributed to this study that tested some classic psychology experiments on AMT.red[*]

    - Crump, M. J., McDonnell, J. V., & Gureckis, T. M. (2013). [Evaluating Amazon's Mechanical Turk as a tool for experimental behavioral research.](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0057410) PloS one, 8(3).

- The goal was to see if classic findings replicate and also check what methods might help increase the quality of the data

- Focus was on reaction time findings that require sustained attention from subjects and precise recording of responses

- And learning experiments involving more cognitive effort 


.footnote[.red[*] For a summary of the paper see this [blog post](http://gureckislab.org/blog/?p=1297)]

---



.task[Part 1: What is AMT?]
# RT Results
.left-column[ 

Stroop

  - People respond faster to congruent (e.g. .blue[blue]) than to incongruent (.red[blue]) items


Task Switching

  - Faster RT if task stays the same (e.g. "Odd or Even?") than when tasks alternate ("Odd or Even?" / "Small or Large?")

Flanker

  - When identifying a target (e.g. "h"), congruent flankers ("hhhhh") lead to faster RTs than incongruent ("ffhhff") ones

]
  

.center[.right-column[
<img src="images/StroopResult.png" width="160">
<img src="images/TaskSwitchingResult.png" width="160">
<img src="images/FlankerResult.png" width="160">
]]  


---

.task[Part 1: What is AMT?]
# RT Results
.left-column[ 

Simon

  - Targets that are spatially compatible with response key are identified faster


Visual Cuing 

  - Stimulus detection is faster for cued  versus uncued targets if cue is presented after short interval (~300ms) and slower after longer interval (~400ms)

Attentional Blink

  - During rapid serial visual presentation visual target detection is impaired if target is displayed 100-500ms after another one


]


.center[.right-column[
<img src="images/SimonResult.png" width="160">
<img src="images/ContCueing.png" width="160">
<img src="images/AttentionalBlink.png" width="160">
]]  

---


.task[Part 1: What is AMT?]
# Subliminal Perception

Masked priming

  - Responding to arrow probes that are either compatibly (prime: >>, probe: >>) or incompatibly primed (prime: <<, probe: >>) at durations of 16, 32, 48, 64, 80, and 96 ms 

  -  Previous finding was that compatibility effects are negative for very short prime durations and positive for longer ones


  .center[
   <img src="images/PrimingResult.png" width="330">
   ]

  - Only partially replicated: no negative compatibility effect

  - Possibility: Lower bound on display duration in browser?

---


.task[Part 1: What is AMT?]
# Category Learning

- Classic Shepard, Hovland, & Jenkins (1961) paper

- Six different ways of classifying the same 8 items (geometric figures with 3 binary dimensions) into deterministic categories

- Generally Type I is learned faster than Type II which is faster that Types III-V, with Type VI the hardest
.center[
<img src="images/abstractstructureSHJ.png" width="150"><img src="images/dimstructureSHJ.png" width="170">
<img src="images/SHJDifficulty.png" width="550">
]

---

.task[Part 1: What is AMT?]
# Results

Although type I was learned easily, performance on all other types was significantly worse than in the original study
<br><br>

.center[
<img src="images/exp8-nosofsky.png" height="290"><img src="images/exp8-amt.png" height="290">
]


---




.task[Part 1: What is AMT?]
# Incentives too low?
There was little difference between high ($2 to $4.50 with bonus), medium ($1.50), and low-payment ($.75) groups
<br><br>

<img src="images/exp9-typeII.png" height="290">
<img src="images/exp9-typeIV.png" height="290">


---


.task[Part 1: What is AMT?]
# Bad instructions?
Including manipulation checks that asked non-trivial questions about the task improved performance on types II to VI
<br><br>

<img src="images/exp10-n-vs-amt.png" height="310">
<img src="images/exp10-l-vs-amt.png" height="310">

---


.task[Part 1: What is AMT?]
# Summary

- Important outcomes
    - Most findings replicated with high fidelity

    - But experiments that rely on very precise and fast stimulus presentation may reach the limit of most browsers

    - Payment didn't have much effect on performance but did affect signup and dropout rates

    - Checking participants' understanding of the task increased data quality considerably
    
<!-- - Note: dropout rates are much higher than in the lab, so maybe consider reporting those if they might interact with performance
 -->


---

.task[Part 1: What is AMT?]
# Replication in other Areas

1. Clinical (Shapiro, et al, 2013)

  - Scores on a range of psychometric tests had high internal consistency and test-retest reliability  (r =.87)

2. Personality (Buhrmeser, et al, 2011)

  - No difference in consistency on personality questionnaires at different payment levels, and absolute levels of consistency and test-retest reliability were high (r = .88)

3. JDM (Paolacci, et al, 2010)
  
  -  Compared classic JDM studies (Asian Disease, Linda, Physicians Problem) on AMT and university subject pool
  - No difference in failure rate of catch trials and pattern of behavior in studies but AMT subjects were more risk averse

---

.task[Part 1: What is AMT?]
# A word on Ethics

- How much should you pay?  
    - As much as the market supports or should it match what you pay in the lab (roughly)?
    - AMT has been criticized for incentivizing companies to outsource labor without adequate compensation (for discussions see for example [here](http://priceonomics.com/who-makes-below-minimum-wage-in-the-mechanical/?utm_source=buffer&utm_campaign=Buffer&utm_content=buffer2d486&utm_medium=twitter), [here](http://economix.blogs.nytimes.com/2013/03/18/the-unregulated-work-of-mechanical-turk/), and [here](http://www.huffingtonpost.com/julian-dobson/mechanical-turk-amazons-underclass_b_2687431.html))

- Can you get IRB approval for your experiment?

    - Arguably, workers have more freedom of choice than lab participants (can stop experiment any time)

    - Wasn't very difficult to add to an existing IRB at NYU


---

.task[Part 1: What is AMT?]
# Be a good requester
 
Workers share information about good and bad requesters:
<br><br>
.center[
<img src="images/Turkopticon.png" height="400">
]
 
---

.task[Part 1: What is AMT?]
# Be a good requester
 
A few recommendations to keep workers happy...

1. Before running, test your experiments on .blue[different browsers] and the sandbox (AMT's testing environment)

1. Give explicit instructions and .blue[show warnings] about closing the page, going back in the browsers, etc.

1. Start collecting data in .blue[small batches]
 
1. If something goes wrong, provide an .blue[alternative payment method] for workers who are unable to finish or submit HIT
    - One way of doing this is to provide a HIT with no content specifically aimed at workers that had problems and pay them using a "bonus"
 

---


name: part2
class: center, middle
 
.task[Part 2: Demo]
 
# Using AMT and the psiTurk Dashboard


---


class: middle
.task[Part 2: Using AMT]

# Two perspectives:
 
1. What is it like to use AMT as a [**Worker**](#terminology)?
 
2. What is it like to use AMT as a [**Requester**](#terminology)?

# Two platforms:

1. The sandbox

    - Worker sandbox ([workersandbox.mturk.com](http://workersandbox.mturk.com))

    - Requester sandbox ([requestersandbox.mturk.com](http://requestersandbox.mturk.com))

2. The real deal ([www.mturk.com](http://www.mturk.com))


---


.task[Part 2: Using AMT]
# What is a HIT ?


- A unit of work, or a "Human Intelligence Task"

- In our experiments, a HIT is typically an entire experiment (as might be done in the lab)
 
- However, it is up to the .orange[**Requester**] to determine what constitutes a HIT
 
- Could be one trial in an experiment, and you just offer the .blue[**Worker**] as many HITs as they care to do
 

---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**]
 
1. Sign up on Amazon's site
 
2. Search for tasks (i.e., HITs) to do online through the AMT interface.
 
3. Worker accepts a HIT.  The task appears in a frame within the AMT website.

    - Accepting a HIT means it is removed from the .orange[**Requester**]'s account. If the Worker decides not to complete the HIT, they have to _return_ it to the .orange[**Requester**] so someone else can do it

4. After completing the HIT, the worker signals to Amazon that they have completed the task
 
5. Payment will be credited to a worker's account some time after completion


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Search view


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Hit details 


---


.task[Part 2: Using AMT]
# Using AMT as a .blue[**Worker**] 

## Ad view


---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
1. Open a Amazon Web Services (AWS) account: [Subscribe here](http://aws.amazon.com)
 
1. Get a Mechanical Turk Requester Account: [Subscribe here](http://requester.mturk.com)
   
1. Create a "dummy" email account which you can use to deal with inevitable issues that come up with Workers
    - Workers ***will*** contact you when they have issues
    - Need a way to deal with this efficiently and quickly (your reputation depends on it!)
     
1. Use the built-in Amazon template or "External Question" approach to design your experiment
 
1. Test your code on the "sandbox" website before going live
 
1. Go live! 
 
 
---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
## Creating a project

 
---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]
 
## Creating a new batch


---


.task[Part 2: Using AMT]
# Using AMT as a .orange[**Requester**]

## Crediting participants


---

 
.task[Part 2: Using AMT]
# AMT's built-in question templates

###Why use them?
- Hosted directly on Amazon's servers
- Good for simple surveys (multiple choice, text fields, radio buttons)
- Easy to use (don't need your own server)
- Tools for helping you "build" the question
- Basic HTML can be included (to format texts, or load data, images, etc. from a separate datafile, for example)
- HITs can be individualized with different content for each


---

.task[Part 2: Using AMT]
# AMT's built-in question templates

###But...
- Dynamic elements cannot be included!
- For those purposes, you want to use AMTs **external question** type


---


.task[Part 2: The mechanics]
# External questions 
 
- You are responsible for:
    - Hosting the experiment
       - **At minimum** you need a computer with a web-accessible HTTP server running (a static IP address may simplify things)
       - You probably want to avoid trying to host it over WiFi 

    - Posting your link to Amazon
     
    - Communicating back to Amazon when the HIT has been completed
     
    - Approving completed HITs for payment in a timely manner
     
    - Dealing quickly with .blue[**Worker**] issues
     
    - Saving your data in a secure place, etc..


---


.task[Part 2: Using AMT]
# External questions

- Interactions with AMT that psiTurk handles:

    - Submit a request for HITs (through the Dashboard)

    - Provide a link to an "ad" that describes the HIT for Workers (this appears within the AMT site when they click 'View a HIT in this group') .red[[1]]

    - After HIT is accepted, provide a link to the experiment .red[[1]]

    - After HIT is complete, provide a button Worker will click to inform Amazon that they have finished. This submits a form with their identifying information to AMT. .red[[2]]


.footnote[
psiTurk hint .red[[1]]: see templates/mturkindex.html<br />
psiTurk hint .red[[2]]: see templates/thanks.html
]


---


.task[Part 2: Using AMT]
# External questions

- When participants see your HIT in the list, they will see basic metadata:

<img src="images/listing.png" width="750" />

- Once they click "View a HIT in this group," they see whatever your site is serving up, in our case this is the default:


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_dashboard.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_awsinfo.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_database.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_server.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_hitconfig.png" width="750" />


---


.task[Part 2: Using AMT]
# Creating a HIT with psiTurk

<img src="images/screenshot_expinfo.png" width="750" />


---


.task[Part 2: Using AMT]
# External questions

## A Participant Accepts

Upon accepting the HIT, they see... whatever you want them to see! In our case,
it's this:

The link to the site. Could be anything! Must link back to Amazon at the end (we'll show you how)


---


.task[Part 2: Using AMT]
# External questions

The last thing served to the participant is a button they can use to inform
Amazon that they have finished the HIT. This submits a form with their
identifying information to `http://www.mturk.com/mturk/externalSubmit`.


---


.task[Part 2: Using AMT]
# External questions

Error upon trying again.


---

.task[Part 2. Using AMT]
# Be a good requester
 
1. Test your experiment on .blue[different browsers]

1. Use the .blue[sandbox] for testing!

1. Provide users with explicit instructions and .blue[show warnings] about closing the page, going back in the browsers, etc.
    - For many more "basic" HITs it's ok to close the window and return another time

1. Start collecting data in .blue[small batches]
 
1. If something goes wrong, provide an .blue[alternative payment method] for workers who did your experiment but were unable to submit the HIT
    - One way of doing this is to provide a HIT with no content specifically aimed at workers that had problems and pay them using a "bonus"


---


class: center, middle
# End of Part 2
### .gray[Time for a break!]
 

---

name: part3
class: center, middle

.task[Part 3: Building dynamic web experiments]

# .blue[Part 3]: Building dynamic web experiments


---

.task[Part 3: Building dynamic web experiments]
# Programming for the web:<br> .blue[The Good]

- Web programming was developed specifically for user interaction.
- Web experiments will run in the lab too!
- Tons of existing resources, both .blue[libraries] and .blue[tutorials].

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: <br>.blue[The Challenge]

Web development has a lot of moving parts:

- .blue[Custom web server] to serve the experiment and interact with Amazon.
- .blue[Database] for the server to store data.
- The actual .blue[web page]:
    - Structure coded in <span class="blue">HTML</span>.
    - Style defined in <span class="blue">CSS</span>.
    - Programs written in <span class="blue">Javascript</span>.

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: <br>.blue[The Challenge]

Web development has a lot of moving parts:

- .blue[Custom web server] to serve the experiment and interact with Amazon. ✓ 
- .blue[Database] for the server to store data. ✓ 
- The actual .blue[web page]:
    - Structure coded in <span class="blue">HTML</span>.
    - Style defined in <span class="blue">CSS</span>.
    - Programs written in <span class="blue">Javascript</span>.

???

PsiTurk completely takes care of the server side for you.

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: <br>.blue[The Challenge]

Web development has a lot of moving parts:

- .blue[Custom web server] to serve the experiment and interact with Amazon. ✓ 
- .blue[Database] for the server to store data. ✓ 
- The actual .blue[web page]:
    - Structure coded in <span class="blue">HTML</span> (`exp.html`).✓ 
    - Style defined in <span class="blue">CSS</span> (`task.css`). ✓ 
    - Programs written in <span class="blue">Javascript</span>.

???

We also provide default html and stylesheet files which you could potentially
leave as they are.

---

.task[Part 3: Building dynamic web experiments]
# Programming for the web: <br>.blue[The Challenge]

Web development has a lot of moving parts:

- .blue[Custom web server] to serve the experiment and interact with Amazon. ✓ 
- .blue[Database] for the server to store data. ✓ 
- The actual .blue[web page]:
    - Structure coded in <span class="blue">HTML</span> (`exp.html`).✓ 
    - Style defined in <span class="blue">CSS</span> (`task.css`). ✓ 
    - Programs written in <strong><span class="red">Javascript</span></strong>.

???
But it's your job to code up the experiment, and you'll have to do that in
Javascript.

---

.task[Part 3: Building dynamic web experiments]
# What is JavaScript?

- For one thing, it has .blue[nothing to do] with Java.
- It is the .blue[only language] that browsers understand natively 
    - (but flash is also an option).
- These slides were compiled in Javascript!
- Javascript's role in a behavioral experiment:
    - Control the <span class="blue">task logic</span>: condition, etc.
    - Control <span class="blue">what subjects see</span> and when.
    - Collect and store <span class="blue">data</span>, then send it to the server.

---

.task[Part 3: Building dynamic web experiments]
# What Javascript can do

- Interact with a web server (to send or request data for instance)
- Track clicks, mouse movements, etc.
- Record response times
- Track changes in focus and window size
- Enable group experiments

---

.task[Part 3: Building dynamic web experiments]
# What Javascript can't do

- Control focus (lock participants in a window)
- Disable browser actions (reloading, going back)
- Saving directly to a file (server interaction is required)

---

.task[Part 3: Building dynamic web experiments]
# Learning Javascript

Lots of resources for learning: E.g., [codecademy.com](http://codecademy.com):

<img src="images/codecademy.png" width="750" />

???

No time to teach javascript

One of the most commonly used languages.

One resource I recommend for beginners is codecademy.

Doesn't assume programming experimence.

Guides you through Javascript interactively.

---

.task[Part 3: Building dynamic web experiments]
# psiturk.js: <br> .blue[A Javascript API]

A small library that will help you:

- Handle page transitions
- Load images
- Record and save data

PsitTurk.js will .blue[automatically]:

- Register if user resizes their Window
- Register if user switches to another tab/window

---

.task[Part 3: Building dynamic web experiments]
# psiturk.js: <br> .blue[Condition and Counterbalance]

```javascript
exp = new Psiturk() // Put this at the top of your file!

// returns subect's server-assigned condition:
exp.taskdata.get("condition")
// returns subect's server-assigned condition:
exp.taskdata.get("counterbalance") 
```

---

.task[Part 3: Building dynamic web experiments]
# psiturk.js: <br> .blue[Fetching resources]

```javascript
exp = new Psiturk()

// Returns HTML from mypage.html, 
// stored in the templates directory
exp.getPage("mypage.html") 
exp.getImage("myimage.jpg") 
```
---

.task[Part 3: Building dynamic web experiments]
# psiturk.js: <br> .blue[Recording and saving data]

```javascript
exp = new Psiturk()

// Records data for the current trial
exp.recordTrialData([1, "L", "CORRECT"]) 
// Records data for the current trial
exp.recordUnstructuredData("Age", 28) 

// Save data to server
exp.saveData()
```

---

.task[Part 3: Building dynamic web experiments]
# Full API doc for reference:

- `psiturk.taskdata` — Data about task/subject
    - `.get("condition")` — Get server-assigned condition number
    - `.get("counterbalance")` — Get server-assigned counterbalance number
    - `.get("useragent")` — Get browser's user agent string.
    - *Includes event listeners to record resize/focus change.*
- `psiturk.getPage` — Retrieve html snippet from `/templates` directory.
- `psiturk.getImage` — Retrieve image from `/static/images` directory.
- `psiturk.recordTrialData` — Accepts list of items, which will be a row in `trialdata.csv` along with the subject's unique identifier
- `psiturk.recordUnstructuredData` — Accepts `key` followed by `value`, encoded along with unique identifier in `questiondata.csv`
- `psiturk.saveData` — Send all data in `taskdata` to the server.
- `psiturk.finishInstructions` — Warn participants not to close window.
- `psiturk.teardownTask` — Allow window to close.


---

.task[Part 3: Building dynamic web experiments]
# Javascript libraries

These are currently dependencies of `psiturk.js`, we also use them in all our experiments.

- [Jquery](http://jquery.org) – Manipulating the website
- [BackboneJS](http://backbonejs.org) – Organizing your code
- [UnderscoreJS](http://underscorejs.org) – Helpful functions, e.g. `_.shuffle()` to shuffle stimuli

<!-- ---

(ANNA: Not sure if this should even go here. Or how we can elaborate on this. It seems good to have it in the section that says what psiturk can do, but it's perhaps a bit strange to do this before we even describe what it is...)


.task[Part 4: Using psiTurk]
# What is it actually?

- A small webserver that you run in Python that makes it very easy to customize

- It runs on a designated port on your computer

- You point the Workers at this url

- Based on three key open-source packages:

<img src="http://flask.pocoo.org/static/logo/flask.png" width="250">  
<img src="http://www.sqlalchemy.org/img/sqla-logo6.gif" width="250">  
[boto](https://github.com/boto/boto) -->

---


name: part4
class: center, middle

.task[Part 3: Using psiTurk]
# .blue[Part 4]: Using psiTurk

---


.task[Part 4: Using psiTurk]
# Installation

**1. System requirements **    

- psiTurk works only on OS X and Linux, Windows is not supported (cloud-based option coming soon!).

<br>
**2. Software dependencies**    

- **Python**

  - If you've never used Python before, we recommend you use the [Enthought Python distribution](http://www.enthought.com/repo/.epd_academic_installers), 
  because it comes with pip, a tool for easily installing Python packages. 

  <br>
  <br>

---


.task[Part 4: Using psiTurk]
# Installation Con't


**2. Software dependencies con't**    

- **pip**

  - If you installed Python using Enthought you don't need to install pip separately. If you want to use another version of Python, you may have to install pip, for example like this:



```bash
curl -O https://raw.github.com/pypa/pip/master/contrib/get-pip.py
[sudo] python get-pip.py
```



**3. Installing psiTurk**

- To install psiTurk simply run:

```bash
pip install git+git://github.com/NYUCCL/psiTurk.git@dev
```

---



.task[Part 4: Using psiTurk]

# Running the Stroop demo

***Goals***

- Post a HIT.
- Use demo code as the basis for a new project


**1. Check out the demo repository**

```bash
git clone git://github.com/NYUCCL/psiTurk.git
```

- or download it as a [zip file](https://github.com/NYUCCL/psiTurk/archive/master.zip).
- You can modify this experiment to create your own.


**2. Move into the demo repository**

```bash
cd psiturk
```

---


.task[Part 4: Using psiTurk]

# Modifying the Stroop task

**3.Launch psiturk**


```bash
psiturk
```


<br>
** Now, Let's get our hands dirty!**

- hands-on dashboard and code demo


---



class: middle, center

# Thanks!


    </textarea>
    <script src="remark.min.js" type="text/javascript">
        
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({ "highlightStyle": "tomorrow-night-bright" });
    </script>
  </body>
</html>
